{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nimport numpy.random as nr\nfrom sklearn import preprocessing\nimport sklearn.model_selection as ms\nfrom sklearn import linear_model\nfrom sklearn import feature_selection as fs\nimport sklearn.decomposition as skde\nimport sklearn.metrics as sklm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport scipy.stats as ss\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.linear_model import ElasticNet, Lasso,  Ridge, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import r2_score\nimport lightgbm as lgb\n#import xgboost as xgb\n\n%matplotlib inline",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df_features = pd.read_csv('train_values.csv')\ndf_labels = pd.read_csv('train_labels.csv')\ndf = df_features.merge(df_labels, on='row_id')\n\ndf.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = df.drop_duplicates(keep = 'last')\ndf = df.drop_duplicates(subset = 'row_id', keep = 'last')\n\ndf.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#droping state_code -1\n\nindexNames = df[df['state_code'] == -1].index\n \n# Delete these row indexes from dataFrame\ndf.drop(indexNames , inplace=True)\n\ndf.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(df.isnull().sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#adjust applicant income missing values by median value\n\ndf['applicant_income'].fillna(df['applicant_income'].median(), inplace=True)\n\nprint(df.isnull().sum())",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#dropping all rows with remaining missing values\nindexNames = df[df['population'].isnull()].index \ndf.drop(indexNames , inplace=True)\n\ni2 = df[df['minority_population_pct'].isnull()].index \ndf.drop(i2 , inplace=True)\n\ni3 = df[df['ffiecmedian_family_income'].isnull()].index \ndf.drop(i3 , inplace=True)\n\ni4 = df[df['tract_to_msa_md_income_pct'].isnull()].index \ndf.drop(i4 , inplace=True)\n\ni5 = df[df['number_of_owner-occupied_units'].isnull()].index \ndf.drop(i5 , inplace=True)\n\ni6 = df[df['number_of_1_to_4_family_units'].isnull()].index \ndf.drop(i6 , inplace=True)\n\ndf.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df.to_csv('cleaned.csv', index = False, header = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2 = pd.read_csv('cleaned.csv')\ndf2.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "##Data Exploration Label\n#cleaning labels above 13 results in deleting some 129 rows\n\ncount = df2[df2['rate_spread'] > 9].index\ndf2.drop(count , inplace=True)\ndf2.describe()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#but improves the skewness to some level\n\ndef hist_plot(vals, lab):\n    ## Distribution plot of values\n    sns.set(style=\"whitegrid\", palette='Blues_r')\n    sns.distplot(vals)\n    plt.title('Histogram of ' + lab)\n    plt.xlabel('Value')\n    plt.ylabel('Density')\n    plt.show()\n    \nsns.set_style(\"whitegrid\")\nhist_plot(df2['rate_spread'], 'Rate Spread')\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#testing squared labels\n\n\n\n#FUNCTION FOR returning 0 for ln(0)\nfrom numpy import errstate,isneginf\na = df2['rate_spread']\nwith errstate(divide='ignore'):\n    res = np.log(a)\nres[isneginf(res)]=0\nhist_plot(res, 'ln RS')\n\n\n#SQUARE LABELS\n#df2['squared_rate_spread'] = np.square(df2['rate_spread'])\n#hist_plot(df2['squared_rate_spread'], 'squared_rate_spread')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#SQUARE LABELS\ndf2['squared_rate_spread'] = np.square(df2['rate_spread'])\nhist_plot(df2['squared_rate_spread'], 'squared_rate_spread')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#data Exploration for features _ Loan Type\n\ndef create_loantype_group (data):\n    loan_conditions = [\n    (data['loan_type'] == 1 ),\n    (data['loan_type'] == 2 ),\n    (data['loan_type'] == 3 ),\n    (data['loan_type'] == 4 )\n    ]\n    loan_choices = ['Conventional', 'FHA', 'VA-Guaranteed', 'FHA/RHS']\n    data['loan_group'] = np.select(loan_conditions, loan_choices)\n\ncreate_loantype_group(df2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#data Exploration for features _ property Type, loan purpose, occupancy, preapproval\n\ndef create_propertytype_group (data):\n    property_conditions = [\n    (data['property_type'] == 1 ),\n    (data['property_type'] == 2 ),\n    (data['property_type'] == 3 )\n     ]\n    property_choices = ['1to4', 'Manufactured', 'MultiFamily']\n    data['property_group'] = np.select(property_conditions, property_choices)\n\ncreate_propertytype_group(df2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_loanpurpose_group (data):\n    conditions = [\n    (data['loan_purpose'] == 1 ),\n    (data['loan_purpose'] == 2 ),\n    (data['loan_purpose'] == 3 )\n    ]\n    choices = ['Purchase', 'Improvement', 'Refinancing']\n    data['loan_purpose_group'] = np.select(conditions, choices)\n\ncreate_loanpurpose_group(df2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_occupancy_group (data):\n    conditions = [\n    (data['occupancy'] == 1 ),\n    (data['occupancy'] == 2 ),\n    (data['occupancy'] == 3 )\n    ]\n    choices = ['Owner Occupied', 'not Owner Occupied', 'Not Applicable']\n    data['occupancy_group'] = np.select(conditions, choices)\n\ncreate_occupancy_group(df2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_preapproval_group (data):\n    conditions = [\n    (data['preapproval'] == 1 ),\n    (data['preapproval'] == 2 ),\n    (data['preapproval'] == 3 )\n    ]\n    choices = ['requested', 'not requested', 'Not Applicable']\n    data['preapproval_group'] = np.select(conditions, choices)\n    \ncreate_preapproval_group(df2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Applicant Features\n\ndef create_applicant_ethnicity_group (data):\n    conditions = [\n    (data['applicant_ethnicity'] == 1 ),\n    (data['applicant_ethnicity'] == 2 ),\n    (data['applicant_ethnicity'] == 3 ),\n    (data['applicant_ethnicity'] == 4 ),\n    (data['applicant_ethnicity'] == 5 )\n    ]\n    choices = ['Hispanic or Latino', 'NOT Hispanic or Latino', 'Information not provided', 'Not Applicable', 'No co-applicant']\n    data['applicant_ethnicity_group'] = np.select(conditions, choices)\n    \ncreate_applicant_ethnicity_group(df2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_applicant_race_group (data):\n    conditions = [\n    (data['applicant_race'] == 1 ),\n    (data['applicant_race'] == 2 ),\n    (data['applicant_race'] == 3 ),\n    (data['applicant_race'] == 4 ),\n    (data['applicant_race'] == 5 ),\n    (data['applicant_race'] == 6 ),\n    (data['applicant_race'] == 7 ),\n    (data['applicant_race'] == 8 )\n    ]\n    choices = ['American Indian or Alaska Native', 'Asian', 'Black or African American','Native Hawaiian or Other Pacific Islander', 'White', 'Information not provided', 'Not Applicable', 'No co-applicant']\n    data['applicant_race_group'] = np.select(conditions, choices)\n    \ncreate_applicant_race_group(df2)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def create_applicant_sex_group (data):\n    conditions = [\n    (data['applicant_sex'] == 1 ),\n    (data['applicant_sex'] == 2 ),\n    (data['applicant_sex'] == 3 ),\n    (data['applicant_sex'] == 4 ),\n    (data['applicant_sex'] == 5 )\n    ]\n    choices = ['Male', 'Female', 'Information not provided', 'Not Applicable', 'Not Applicable']\n    data['applicant_sex_group'] = np.select(conditions, choices)\n    \ncreate_applicant_sex_group(df2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2.dtypes",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2.drop('loan_type', axis = 1, inplace = True)\ndf2.drop('property_type', axis = 1, inplace = True)\ndf2.drop('loan_purpose', axis = 1, inplace = True)\ndf2.drop('occupancy', axis = 1, inplace = True)\ndf2.drop('preapproval', axis = 1, inplace = True)\ndf2.drop('applicant_ethnicity', axis = 1, inplace = True)\ndf2.drop('applicant_race', axis = 1, inplace = True)\ndf2.drop('applicant_sex', axis = 1, inplace = True)\ndf2.drop('squared_rate_spread', axis = 1, inplace = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#counts for each object feature\ndef count_unique(df2, cols):\n    for col in cols:\n        print('\\n' + 'For column ' + col)\n        print(df2[col].value_counts())\n\ncat_cols = ['loan_group','property_group','loan_purpose_group','occupancy_group','preapproval_group','applicant_ethnicity_group','applicant_race_group','applicant_sex_group']\n\ncount_unique(df2, cat_cols)\ndf2.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#violin plot\ndef plot_violin(df2, cols, col_y, title):\n    for col in cols:\n        sns.set(style=\"whitegrid\")\n        sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n        sns.violinplot(col, col_y, data=df2)\n        plt.xlabel(col) # Set text for the x axis\n        plt.ylabel(col_y)# Set text for y axis\n        plt.title(title + ' by ' + col)\n        plt.show()\n        \nplot_violin(df2, cat_cols, 'rate_spread', 'rate_spread_I')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "num_cols = ['loan_amount', 'applicant_income', 'population', 'minority_population_pct', 'ffiecmedian_family_income', 'tract_to_msa_md_income_pct', 'number_of_owner-occupied_units', 'number_of_1_to_4_family_units'  ] \n\ndef plot_density_hist(df2, cols, bins = 10, hist = False):\n    for col in cols:\n        sns.set(style=\"whitegrid\", palette='Blues_r')\n        sns.distplot(df[col], bins = bins, rug=True, hist = hist)\n        plt.title('Histogram of ' + col) # Give the plot a main title\n        plt.xlabel(col) # Set text for the x axis\n        plt.ylabel('Frequency')# Set text for y axis\n        plt.show()\n        \nplot_density_hist(df2, num_cols, bins = 20, hist = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "sns.set(style=\"whitegrid\", palette='Blues_r')\nsns.pairplot(df2[num_cols])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#correlation matrix\n\ncorrs = df2[num_cols].corr()\n\n## Create the hierarchical clustering model\ndist = sch.distance.pdist(corrs)   # vector of pairwise distances using correlations\nlinkage = sch.linkage(dist, method='complete') # Compute the linkages for the clusters\nind = sch.fcluster(linkage, 0.5*dist.max(), 'distance')  # Apply the clustering algorithm\n\n## Order the columns of the correlaton matrix according to the hierarchy\ncolumns = [corrs.columns.tolist()[i] for i in list((np.argsort(ind)))]  # Order the names for the result\ncorrs_clustered = corrs.reindex(columns) ## Reindex the columns following the heirarchy \n\n## Correlation Plot\ncorrs_clustered.style.background_gradient().set_precision(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "\nnum_corrs = df2[num_cols].corr()\nf,ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(num_corrs, annot=True, square=True, linewidths=.1, fmt= '.1f',ax=ax, \n           cmap=\"RdBu\")\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Aggregating the Categorial values as below\n\n1. Property Group : combining Manufactured and MultiFamily as Manuf&Multi\n2. Occupancy Group : combining Not Applicable and not Owner Occupied as not Applicable&OwnerOccupied\n3. Applicant ethnicity Group: combining Not Applicable to Info Not provided as InfoNotPresent\n4. Applicant Race Group: combining Not Applicable to Native Hawaian as NH&InfoNotPresent\n5. Applicant Sex Group: combining Not applicable to Info Not Provided as InfoNotPresent "
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def plot_box_1(df, col, col_y, title):\n    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n    sns.boxplot(col, col_y, data=df)\n    plt.xlabel(col) # Set text for the x axis\n    plt.ylabel(col_y)# Set text for y axis\n    plt.title(title + ' by ' + col)\n    plt.show()\n    \ndef plot_violin_1(df, col, col_y, title):\n    sns.set_style(\"whitegrid\")\n    sns.set_palette(\"Set1\", n_colors=7, desat=.7)\n    fig, ax = plt.subplots(figsize=(11,8))\n    sns.violinplot(col, col_y, data=df)\n    plt.xlabel(col) # Set text for the x axis\n    plt.ylabel(col_y)# Set text for y axis\n    plt.title(title + ' by ' + col)\n    plt.show()\n\nplot_violin_1(df2, 'property_group', 'rate_spread', 'RS')\nplot_violin_1(df2, 'occupancy_group', 'rate_spread', 'RS')\nplot_violin_1(df2, 'applicant_ethnicity_group', 'rate_spread', 'RS')\nplot_violin_1(df2, 'applicant_race_group', 'rate_spread', 'RS')\nplot_violin_1(df2, 'applicant_sex_group', 'rate_spread', 'RS')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#propertygroup aggregations\nproperty_categories = {'1to4':'1to4', 'Manufactured':'Manuf&Multi',\n                       'MultiFamily':'Manuf&Multi'}\ndf2['property_group'] = [property_categories[x] for x in df2['property_group']]\n\n\n#occupancy group aggregations\noccupancy_group_categories = {'Owner Occupied':'Owner Occupied', 'not Owner Occupied':'notApplicable&OwnerOccupied', 'Not Applicable':'notApplicable&OwnerOccupied'}\ndf2['occupancy_group'] = [occupancy_group_categories[x] for x in df2['occupancy_group']]\n\n\n#ethnicity group aggregations\napplicant_ethnicity_group_categories = {'NOT Hispanic or Latino':'NOT Hispanic or Latino', 'Hispanic or Latino':'Hispanic or Latino', 'Information not provided':'InfoNotPresent', 'Not Applicable':'InfoNotPresent'}\ndf2['applicant_ethnicity_group'] = [applicant_ethnicity_group_categories[x] for x in df2['applicant_ethnicity_group']]\n\n\n#applicant_race_group aggregations\napplicant_race_group_categories = {'White':'White', 'Black or African American':'Black or African American', 'Information not provided':'InfoNotPresent', 'Asian':'Asian', 'American Indian or Alaska Native':'American Indian or Alaska Native', 'Native Hawaiian or Other Pacific Islander':'Native Hawaiian or Other Pacific Islander', 'Not Applicable':'InfoNotPresent'}\ndf2['applicant_race_group'] = [applicant_race_group_categories[x] for x in df2['applicant_race_group']]\n\n\napplicant_sex_group_categories = {'Male':'Male', 'Female':'Female',\n                                      'Information not provided':'InfoNotPresent',\n                                      'Not Applicable':'InfoNotPresent'}\ndf2['applicant_sex_group'] = [applicant_sex_group_categories[x] for x in df2['applicant_sex_group']]\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def count_unique(df2, cols):\n    for col in cols:\n        print('\\n' + 'For column ' + col)\n        print(df2[col].value_counts())\n\ncat_cols1 = ['property_group','occupancy_group','applicant_ethnicity_group','applicant_race_group','applicant_sex_group']\n\ncount_unique(df2, cat_cols1)\ndf2.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "plot_violin_1(df2, 'property_group', 'rate_spread', 'RS')\nplot_violin_1(df2, 'occupancy_group', 'rate_spread', 'RS')\nplot_violin_1(df2, 'applicant_ethnicity_group', 'rate_spread', 'RS')\nplot_violin_1(df2, 'applicant_race_group', 'rate_spread', 'RS')\nplot_violin_1(df2, 'applicant_sex_group', 'rate_spread', 'RS')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "attempting to transform the numeric features to log function for even distribution"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hist_plot(df2['loan_amount'],'loan_amount')\ndf2['ln_loan_amount'] = np.log(df2['loan_amount'])\nhist_plot(df2['ln_loan_amount'], 'natural log loan')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df2['ln_applicant_income'] = np.log(df2['applicant_income'])\ndf2['ln_population'] = np.log(df2['population'])\ndf2['ln_ffiecmedian_family_income'] = np.log(df2['ffiecmedian_family_income'])\ndf2['ln_number_of_owner-occupied_units'] = np.log(df2['number_of_owner-occupied_units'])\ndf2['ln_number_of_1_to_4_family_units'] = np.log(df2['number_of_1_to_4_family_units'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "hist_plot(df2['ln_applicant_income'], 'natural log applicant income')\nhist_plot(df2['ln_population'], 'natural log population')\nhist_plot(df2['ln_ffiecmedian_family_income'], 'natural log mdeian family_incom')\nhist_plot(df2['ln_number_of_owner-occupied_units'], 'natural log owner-occupied_units')\nhist_plot(df2['ln_number_of_1_to_4_family_units'], 'natural log 1_to_4_family_units')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Model Matrix Preparation\nWe are predicting the label rate_spread with both categorical and numeric features. Categorical features are one-hot encoded, and joint back with the numeric features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "Labels = np.array(df2['rate_spread'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def encode_string(cat_features):\n    ## Now, apply one hot encoding\n    ohe = preprocessing.OneHotEncoder(categories='auto')\n    encoded = ohe.fit_transform(cat_features.values.reshape(-1,1)).toarray()\n    pdfn = ohe.get_feature_names()\n    print(pdfn)\n    return encoded\n\nfeatures_cat_cols = ['co_applicant','property_group','loan_purpose_group','occupancy_group','preapproval_group','applicant_ethnicity_group',\n                     'applicant_race_group','applicant_sex_group']\n\nFeatures = encode_string(df2['loan_group'])\nfor col in features_cat_cols:\n    temp = encode_string(df2[col])\n    Features = np.concatenate([Features, temp], axis = 1)\n    \nprint(Features.shape)\nprint(Features[:2, :])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "features_num_cols = ['ln_loan_amount', 'ln_applicant_income','ln_population','ln_ffiecmedian_family_income','ln_number_of_owner-occupied_units',\n                     'ln_number_of_1_to_4_family_units','minority_population_pct','tract_to_msa_md_income_pct']\nFeatures = np.concatenate([Features, np.array(df2[features_num_cols])], axis = 1)\nprint(Features.shape)\nprint(Features[:2, :])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Model selection\n\n\nEliminate low variance features\n\nWe eliminate features with low variance using the scikit-learn VarianceThreshold based function. A p = 0.95 was found to optimize the model."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(Features.shape)\n\n## Define the variance threhold and fit the threshold to the feature array.\nsel = fs.VarianceThreshold(threshold=(.95 * (1 - .95)))\nFeatures_reduced = sel.fit_transform(Features)\nprint(sel.get_support())\n\n## Print the support and shape for the transformed features\nprint(Features_reduced.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Recursive feature elimination\n\nWe used the scikit-learn RFECV function to determine which features to retain using a cross validation method, with the ridge regression model:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "## Reshape the Label array\nLabels = Labels.reshape(Labels.shape[0],)\n\n## Set folds for nested cross validation\nnr.seed(562)\nfeature_folds = ms.KFold(n_splits=10, shuffle = True)\n\n## Define the model\nlin_mod_l2 = linear_model.Ridge()\n\n## Perform feature selection by CV with high variance features only\nnr.seed(265)\nselector = fs.RFECV(estimator = lin_mod_l2, cv = feature_folds, scoring = 'r2')\nselector = selector.fit(Features_reduced, Labels)\nprint(selector.support_)\nprint(selector.ranking_)\n\nFeatures_reduced = selector.transform(Features_reduced)\nprint(Features_reduced.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Split the Dataset\n\nWith the model matrix constructed, we now create randomly sampled training and test data sets in a 7:3 ratio"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(265)\nindx = range(Features.shape[0])\nindx = ms.train_test_split(indx, test_size = 0.3)\nx_train = Features_reduced[indx[0],:]\ny_train = np.ravel(Labels[indx[0]])\nx_test = Features_reduced[indx[1],:]\ny_test = np.ravel(Labels[indx[1]])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Rescale numeric features\n\nWe use the StandardScaler to z-score scale the numeric features."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scaler = preprocessing.StandardScaler().fit(x_train[:,27:])\nx_train[:,27:] = scaler.transform(x_train[:,27:])\nx_test[:,27:] = scaler.transform(x_test[:,27:])\nprint(x_train[:2,])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Construct the Regression Models\n\nDifferent regression models are tested with optimized hyperparameters. We first fit the models with pre-selected parameters to our train data, and fine-tune them recursively. Here we will test Gradient Boosting Regressor, XGboost Regressor and LGB Regressor, the current best-performing algorithms for data science competitions."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "GBoost = GradientBoostingRegressor()\nmodel_lgb = lgb.LGBMRegressor(objective='regression', num_leaves = 32,\n                              learning_rate=0.01)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "nr.seed(265)\ninside = ms.KFold(n_splits=5, shuffle = True)\nnr.seed(562)\noutside = ms.KFold(n_splits=5, shuffle = True)\n\nnr.seed(2652)\n## Define the dictionary for the grid search and the model object to search on\nparam_grid = {'n_estimators': [2000, 3000]}\n\n## Perform the grid search over the parameters\ngsearch = ms.GridSearchCV(estimator = model_lgb, param_grid = param_grid, \n                      cv = inside, # Use the inside folds\n                      scoring = 'r2',\n                      return_train_score = True)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "gsearch.fit(Features_reduced, Labels)\ngsearch.best_params_, gsearch.best_score_",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "GBoost = GradientBoostingRegressor(n_estimators= 2000, learning_rate=0.01,\n                                   max_depth=5, max_features='sqrt',\n                                   min_samples_leaf=7, min_samples_split=15, \n                                   loss='ls', random_state = 1)\n\nmodel_lgb = lgb.LGBMRegressor(objective='regression', num_leaves = 32,\n                              learning_rate=0.01, n_estimators=2100, \n                              bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.4,\n                              min_data_in_leaf = 5,  \n                              feature_fraction_seed=3, bagging_seed=2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Validation function\nn_folds = 5\n\ndef r2_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train)\n    r2 = cross_val_score(model, x_train, y_train, scoring=\"r2\", cv = kf)\n    return(r2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "score = r2_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "score = r2_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def rsquare(y, y_pred):\n    return r2_score(y, y_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "GBoost.fit(x_train, y_train)\ngb_pred = GBoost.predict(x_test)\nprint(rsquare(y_test, gb_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model_lgb.fit(x_train, y_train)\nlgb_pred = model_lgb.predict(x_test)\nprint(rsquare(y_test, lgb_pred))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "lgb_pred[lgb_pred < 0] = 0\nlgb_pred[lgb_pred > 7] = 7",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def print_metrics(y_true, y_predicted, n_parameters):\n    ## First compute R^2 and the adjusted R^2\n    r2 = sklm.r2_score(y_true, y_predicted)\n    r2_adj = r2 - (n_parameters - 1)/(y_true.shape[0] - n_parameters) * (1 - r2)\n    \n    ## Print the usual metrics and the R^2 values\n    print('Mean Square Error      = ' + str(sklm.mean_squared_error(y_true, y_predicted)))\n    print('Root Mean Square Error = ' + str(math.sqrt(sklm.mean_squared_error(y_true, y_predicted))))\n    print('Mean Absolute Error    = ' + str(sklm.mean_absolute_error(y_true, y_predicted)))\n    print('Median Absolute Error  = ' + str(sklm.median_absolute_error(y_true, y_predicted)))\n    print('R^2                    = ' + str(r2))\n    print('Adjusted R^2           = ' + str(r2_adj))\n   \nprint_metrics(y_test, lgb_pred, 28)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def resid_qq(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ss.probplot(resids.flatten(), plot = plt)\n    ax.get_lines()[0].set_marker('+')\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n    \nresid_qq(y_test, lgb_pred)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def resid_plot(y_test, y_score):\n    ## first compute vector of residuals. \n    resids = np.subtract(y_test.reshape(-1,1), y_score.reshape(-1,1))\n    ## now make the residual plots\n    sns.regplot(y_score, resids, fit_reg=False, marker=\"+\", scatter_kws={'alpha':0.5})\n    plt.title('Residuals vs. predicted values')\n    plt.xlabel('Predicted values')\n    plt.ylabel('Residual')\n\nresid_plot(y_test, lgb_pred)",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}